{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "437ea841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "#from transformers import AutoTokenizer\n",
    "from transformers import DistilBertForTokenClassification\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score,f1_score, precision_score, recall_score\n",
    "import re\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "#import part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2fb9c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 43821 #columns of dataset\n",
    "df = pd.read_csv(\"NER_dataset.csv\", encoding=\"cp949\").sample(frac=1)[:N] #load dataset\n",
    "\n",
    "#change field name\n",
    "df.rename(columns = {'text':'sentence', 'labels':'tags'}, inplace = True)\n",
    "\n",
    "#split train, dev, test data (dev data is used for cross validation)\n",
    "df_train, df_dev, df_test = np.split(df.sample(frac=1, random_state=0), [int(.8 * len(df)), int(.9 * len(df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cc68dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22920</th>\n",
       "      <td>ADMISSION DATE :</td>\n",
       "      <td>O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10027</th>\n",
       "      <td>SOCIAL HISTORY :</td>\n",
       "      <td>O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8066</th>\n",
       "      <td>This is the baby 's discharge feedings and he ...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32300</th>\n",
       "      <td>Abd : Soft , NT/ND , NABS</td>\n",
       "      <td>O O O O B-PR O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30688</th>\n",
       "      <td>ABD : soft ND/NT +BS</td>\n",
       "      <td>O O O B-PR O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  \\\n",
       "22920                                   ADMISSION DATE :   \n",
       "10027                                   SOCIAL HISTORY :   \n",
       "8066   This is the baby 's discharge feedings and he ...   \n",
       "32300                          Abd : Soft , NT/ND , NABS   \n",
       "30688                               ABD : soft ND/NT +BS   \n",
       "\n",
       "                                                    tags  \n",
       "22920                                             O O O   \n",
       "10027                                             O O O   \n",
       "8066   O O O O O O O O O O O O O O O O O O O O O O O ...  \n",
       "32300                                  O O O O B-PR O O   \n",
       "30688                                      O O O B-PR O   "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "#show head part of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fa71c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11534</th>\n",
       "      <td>no masses , neck revealed lymphadenopathy .</td>\n",
       "      <td>O B-PR O O O B-PR O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>Post thoracotomy syndrome .</td>\n",
       "      <td>B-PR I-PR I-PR O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21815</th>\n",
       "      <td>For possible COPD , pt was started on steroids...</td>\n",
       "      <td>O O B-PR O O O O O B-TR O B-TR O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28621</th>\n",
       "      <td>The patient refused an LP .</td>\n",
       "      <td>O O O B-TE I-TE O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31980</th>\n",
       "      <td>2012-06-07 09:55 PM BLOOD PT - 12.8 PTT - 25.7...</td>\n",
       "      <td>O O O B-TE B-TE O O B-TE O O B-TE O O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  \\\n",
       "11534        no masses , neck revealed lymphadenopathy .   \n",
       "955                          Post thoracotomy syndrome .   \n",
       "21815  For possible COPD , pt was started on steroids...   \n",
       "28621                        The patient refused an LP .   \n",
       "31980  2012-06-07 09:55 PM BLOOD PT - 12.8 PTT - 25.7...   \n",
       "\n",
       "                                         tags  \n",
       "11534                    O B-PR O O O B-PR O   \n",
       "955                         B-PR I-PR I-PR O   \n",
       "21815   O O B-PR O O O O O B-TR O B-TR O O O   \n",
       "28621                      O O O B-TE I-TE O   \n",
       "31980  O O O B-TE B-TE O O B-TE O O B-TE O O   "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()\n",
    "#show tail part of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd7e26e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O', 'I-TR', 'I-PR', 'B-TR', 'I-TE', 'B-PR', 'B-TE'}\n",
      "{'B-PR': 0, 'B-TE': 1, 'B-TR': 2, 'I-PR': 3, 'I-TE': 4, 'I-TR': 5, 'O': 6}\n"
     ]
    }
   ],
   "source": [
    "# tansfrom label to list (delimiter is \" \")\n",
    "labels = [i.split() for i in df['tags'].values.tolist()]\n",
    "\n",
    "# count the number of label\n",
    "unique_labels = set()\n",
    "\n",
    "for lb in labels:\n",
    "    [unique_labels.add(i) for i in lb if i not in unique_labels]\n",
    " \n",
    "print(unique_labels)\n",
    "# {'B-TE', 'I-TR', 'I-PR', 'I-TE', 'O', 'B-TR', 'B-PR'}\n",
    "\n",
    "# mappingg unique_lables to their id\n",
    "labels_to_ids = {k: v for v, k in enumerate(sorted(unique_labels))}\n",
    "ids_to_labels = {v: k for v, k in enumerate(sorted(unique_labels))}\n",
    "print(labels_to_ids)\n",
    "# {'B-PR': 0, 'B-TE': 1, 'B-TR': 2, 'I-PR': 3, 'I-TE': 4, 'I-TR': 5, 'O': 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da4c180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main class for doing NER with Distilbert\n",
    "\n",
    "class DistilbertNER(nn.Module):\n",
    "  \n",
    "    def __init__(self, tokens_dim): #constructor \n",
    "        super(DistilbertNER,self).__init__()\n",
    "    \n",
    "        if type(tokens_dim) != int:\n",
    "            raise TypeError('tokens_dim should be an integer')\n",
    "\n",
    "        if tokens_dim <= 0:\n",
    "              raise ValueError('Classification layer dimension should be at least 1')\n",
    "\n",
    "        self.pretrained = DistilBertForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels = tokens_dim) #for using distilBERT model\n",
    "        #self.pretrained = AutoModelForMaskedLM.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\", num_labels = tokens_dim) #for using bioBERT model\n",
    "        #set the output of each token classifier = unique_lables\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels = None): #compute loss\n",
    "\n",
    "        if labels == None:\n",
    "            out = self.pretrained(input_ids = input_ids, attention_mask = attention_mask )\n",
    "            return out #return without label\n",
    "\n",
    "        out = self.pretrained(input_ids = input_ids, attention_mask = attention_mask , labels = labels)\n",
    "        return out #return with label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3cac910",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generalized function for get information from dataset\n",
    "\n",
    "class NerDataset(torch.utils.data.Dataset):\n",
    "  \n",
    "  def __init__(self, df):\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "      raise TypeError('Input should be a dataframe')\n",
    "    \n",
    "    if \"tags\" not in df.columns or \"sentence\" not in df.columns:\n",
    "      raise ValueError(\"Dataframe should contain 'tags' and 'sentence' columns\")\n",
    "    \n",
    "    tags_list = [i.split() for i in df[\"tags\"].values.tolist()]\n",
    "    texts = df[\"sentence\"].values.tolist()\n",
    "    \n",
    "    #for change float(nan) -> string(\"nan\")\n",
    "    i = 0\n",
    "    for string in texts:\n",
    "        i += 1\n",
    "        if(isinstance(string, float)):\n",
    "            texts[i - 1] = \"nan\"\n",
    "\n",
    "    self.texts = [tokenizer(text, padding = \"max_length\", truncation = True, return_tensors = \"pt\") for text in texts]\n",
    "    self.labels = [match_tokens_labels(text, tags) for text,tags in zip(self.texts, tags_list)]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    batch_text = self.texts[idx]\n",
    "    batch_labels = self.labels[idx]\n",
    "\n",
    "    return batch_text, torch.LongTensor(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47e4bcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsTracking():\n",
    "  \"\"\"\n",
    "  In order make the train loop lighter I define this class to track all the metrics that we are going to measure for our model.\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "\n",
    "    self.total_acc = 0\n",
    "    self.total_f1 = 0\n",
    "    self.total_precision = 0\n",
    "    self.total_recall = 0\n",
    "\n",
    "  def update(self, predictions, labels , ignore_token = -100):\n",
    "    '''\n",
    "    Call this function every time you need to update your metrics.\n",
    "    Where in the train there was a -100, were additional token that we dont want to label, so remove them.\n",
    "    If we flatten the batch its easier to access the indexed = -100\n",
    "    '''  \n",
    "    predictions = predictions.flatten()\n",
    "    labels = labels.flatten()\n",
    "    \n",
    "    predictions = predictions[labels != ignore_token]\n",
    "    labels = labels[labels != ignore_token]\n",
    "\n",
    "    predictions = predictions.to(\"cpu\")\n",
    "    labels = labels.to(\"cpu\")\n",
    "\n",
    "    acc = accuracy_score(labels,predictions)\n",
    "    f1 = f1_score(labels, predictions, average = \"macro\")\n",
    "    precision = precision_score(labels, predictions, average = \"macro\")\n",
    "    recall = recall_score(labels, predictions, average = \"macro\")\n",
    "\n",
    "    self.total_acc  += acc\n",
    "    self.total_f1 += f1\n",
    "    self.total_precision += precision\n",
    "    self.total_recall  += recall\n",
    "\n",
    "  def return_avg_metrics(self,data_loader_size):\n",
    "    n = data_loader_size\n",
    "    metrics = {\n",
    "        \"acc\": round(self.total_acc / n ,3), \n",
    "        \"f1\": round(self.total_f1 / n, 3), \n",
    "        \"precision\" : round(self.total_precision / n, 3), \n",
    "        \"recall\": round(self.total_recall / n, 3)\n",
    "          }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f8f46b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create label\n",
    "\n",
    "def tags_2_labels(tags : str, tag2idx : dict):\n",
    "  return [tag2idx[tag] if tag in tag2idx else unseen_label for tag in tags.split()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16653275",
   "metadata": {},
   "outputs": [],
   "source": [
    "#map words to tag\n",
    "\n",
    "def tags_mapping(tags_series : pd.Series):\n",
    "  if not isinstance(tags_series, pd.Series):\n",
    "      raise TypeError('Input should be a padas Series')\n",
    "\n",
    "  unique_tags = set()\n",
    "  \n",
    "  for tag_list in df_train[\"tags\"]:\n",
    "    for tag in tag_list.split():\n",
    "      unique_tags.add(tag)\n",
    "\n",
    "  tag2idx = {k:v for v,k in enumerate(sorted(unique_tags))}\n",
    "  idx2tag = {k:v for v,k in tag2idx.items()}\n",
    "\n",
    "  unseen_label = tag2idx[\"O\"]\n",
    "\n",
    "  return tag2idx, idx2tag, unseen_label, unique_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a66e276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-100 means CLS or PAD\n",
    "\n",
    "def match_tokens_labels(tokenized_input, tags, ignore_token = -100):\n",
    "        word_ids = tokenized_input.word_ids()\n",
    "\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "\n",
    "            if word_idx is None:\n",
    "                label_ids.append(ignore_token)\n",
    "\n",
    "            else :\n",
    "                try:\n",
    "                  reference_tag = tags[word_idx]\n",
    "                  label_ids.append(tag2idx[reference_tag])\n",
    "                except:\n",
    "                  label_ids.append(ignore_token)\n",
    "            \n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        return label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6845bbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train & evaluation function\n",
    "\n",
    "def train_loop(model, train_dataset, dev_dataset, optimizer,  batch_size, epochs):\n",
    "  \n",
    "  train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "  dev_dataloader = DataLoader(dev_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "  model = model.to(device)\n",
    "\n",
    "  for epoch in range(epochs) : \n",
    "    \n",
    "    train_metrics = MetricsTracking()\n",
    "    total_loss_train = 0\n",
    "\n",
    "    model.train() #core function for train\n",
    "\n",
    "    for train_data, train_label in tqdm(train_dataloader):\n",
    "\n",
    "      train_label = train_label.to(device)\n",
    "\n",
    "      mask = train_data['attention_mask'].squeeze(1).to(device)\n",
    "      input_id = train_data['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      \n",
    "      output = model(input_id, mask, train_label)\n",
    "      loss, logits = output.loss, output.logits\n",
    "      predictions = logits.argmax(dim= -1) \n",
    "\n",
    "      train_metrics.update(predictions, train_label)\n",
    "      total_loss_train += loss.item()\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "    \n",
    "    model.eval() #core function for evaluation\n",
    "\n",
    "    dev_metrics = MetricsTracking()\n",
    "    total_loss_dev = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      for dev_data, dev_label in dev_dataloader:\n",
    "\n",
    "        dev_label = dev_label.to(device)\n",
    "\n",
    "        mask = dev_data['attention_mask'].squeeze(1).to(device)\n",
    "        input_id = dev_data['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "        output = model(input_id, mask, dev_label)\n",
    "        loss, logits = output.loss, output.logits\n",
    "\n",
    "        predictions = logits.argmax(dim= -1)     \n",
    "\n",
    "        dev_metrics.update(predictions, dev_label)\n",
    "        total_loss_dev += loss.item()\n",
    "    \n",
    "    train_results = train_metrics.return_avg_metrics(len(train_dataloader))\n",
    "    dev_results = dev_metrics.return_avg_metrics(len(dev_dataloader))\n",
    "\n",
    "    print(f\"TRAIN \\nLoss: {total_loss_train / len(train_dataset)} \\nMetrics {train_results}\\n\" ) \n",
    "    print(f\"VALIDATION \\nLoss {total_loss_dev / len(dev_dataset)} \\nMetrics{dev_results}\\n\" )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ed86fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create mapp between label and tag\n",
    "tag2idx, idx2tag , unseen_label, unique_tags = tags_mapping(df_train[\"tags\"])\n",
    "\n",
    "#change label to tag (surplus label will be changed to \"O\" tag)\n",
    "for df in [df_train, df_dev, df_test]:\n",
    "  df[\"labels\"] = df[\"tags\"].apply(lambda tags : tags_2_labels(tags, tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35fd3ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\") #needed to useing distilBERT model\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\", model_max_length = 512) #needed to useing bioBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22643578",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df_train[\"sentence\"].values.tolist()\n",
    "\n",
    "#for change float(nan) -> string(\"nan\")\n",
    "\n",
    "i = 0\n",
    "for string in text:\n",
    "    i += 1\n",
    "    if(isinstance(string, float)):\n",
    "        text[i - 1] = \"nan\"\n",
    "\n",
    "text_tokenized = tokenizer(text, padding = \"max_length\", truncation = True, return_tensors = \"pt\")\n",
    "#text_tokenized = text_tokenized.remove_columns(books_dataset[\"train\"].column_names) #needed to useing bioBERT tokenizer\n",
    "\n",
    "#map tokens to corresponding words\n",
    "word_ids = text_tokenized.word_ids()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b79c747c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL is not exist so new MODEL will be created\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilbertNER(\n",
       "  (pretrained): DistilBertForTokenClassification(\n",
       "    (distilbert): DistilBertModel(\n",
       "      (embeddings): Embeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (transformer): Transformer(\n",
       "        (layer): ModuleList(\n",
       "          (0): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerBlock(\n",
       "            (attention): MultiHeadSelfAttention(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (ffn): FFN(\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): GELUActivation()\n",
       "            )\n",
       "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DistilbertNER(len(unique_tags))\n",
    "learn = False\n",
    "\n",
    "#determine whether new train & learn is needed or not\n",
    "\n",
    "try :\n",
    "    model = torch.load(\"NER_model\", map_location=torch.device('cpu'))\n",
    "except FileNotFoundError as e : \n",
    "    print(\"MODEL is not exist so new MODEL will be created\")\n",
    "    learn = True\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdd320b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2191/2191 [07:38<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN \n",
      "Loss: 0.02761397135081828 \n",
      "Metrics {'acc': 0.85, 'f1': 0.638, 'precision': 0.716, 'recall': 0.631}\n",
      "\n",
      "VALIDATION \n",
      "Loss 0.020843860015718564 \n",
      "Metrics{'acc': 0.886, 'f1': 0.716, 'precision': 0.793, 'recall': 0.704}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2191/2191 [07:34<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN \n",
      "Loss: 0.01842606408548206 \n",
      "Metrics {'acc': 0.897, 'f1': 0.758, 'precision': 0.82, 'recall': 0.748}\n",
      "\n",
      "VALIDATION \n",
      "Loss 0.015522936243004501 \n",
      "Metrics{'acc': 0.914, 'f1': 0.794, 'precision': 0.843, 'recall': 0.785}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2191/2191 [07:38<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN \n",
      "Loss: 0.014188383151187839 \n",
      "Metrics {'acc': 0.919, 'f1': 0.813, 'precision': 0.861, 'recall': 0.805}\n",
      "\n",
      "VALIDATION \n",
      "Loss 0.015683170521716 \n",
      "Metrics{'acc': 0.914, 'f1': 0.803, 'precision': 0.839, 'recall': 0.807}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2191/2191 [07:34<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN \n",
      "Loss: 0.011856908196162202 \n",
      "Metrics {'acc': 0.931, 'f1': 0.842, 'precision': 0.883, 'recall': 0.834}\n",
      "\n",
      "VALIDATION \n",
      "Loss 0.01612855273752466 \n",
      "Metrics{'acc': 0.917, 'f1': 0.812, 'precision': 0.829, 'recall': 0.831}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████████████████████████████████████████████████████████████████████████▌| 2179/2191 [07:32<00:02,  4.76it/s]"
     ]
    }
   ],
   "source": [
    "#set the hyperparameters\n",
    "\n",
    "train_dataset = NerDataset(df_train)\n",
    "dev_dataset = NerDataset(df_dev)\n",
    "\n",
    "lr = 1e-2\n",
    "optimizer = SGD(model.parameters(), lr=lr, momentum = 0.9)  \n",
    "\n",
    "#MAIN\n",
    "parameters = {\n",
    "    \"model\": model,\n",
    "    \"train_dataset\": train_dataset,\n",
    "    \"dev_dataset\" : dev_dataset,\n",
    "    \"optimizer\" : optimizer,\n",
    "    \"batch_size\" : 16,\n",
    "    \"epochs\" : 5\n",
    "}\n",
    "\n",
    "if learn == True: #do train & test if NER_model not exist\n",
    "    train_loop(**parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52be4953",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if learn == True: #do export the result of train if NER_model not exist\n",
    "    torch.save(model, \"NER_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
